This folder contains a curated collection of **14 representative program samples** used to evaluate the performance of the **TBFV4J-CSC** tool in comparison with the **TBFV4J-Ran** tool, comprising a total of **92 test results**. These programs cover a variety of common functional logic patterns and control structures, offering a well-rounded benchmark to assess the effectiveness of TBFV4J-CSC across different verification scenarios. The dataset enables a comprehensive comparison between TBFV4J-CSC and the baseline random strategy (TBFV4J-Ran) in terms of bug detection capability, the number of test cases generated, and execution efficiency.

The provided experimental statistics include the following aspects:

- The verification result for each program under both tools (e.g., **Killed Mutants** or **Not Found**), serving as an indicator of fault detection effectiveness;
- The number of test cases generated during verification, reflecting the adequacy of path exploration and the strength of test coverage;
- The average execution time per program for both tools, used to evaluate performance efficiency;
- For programs containing semantic faults, representative **counterexamples** automatically generated by the tools are included to demonstrate their capability in detecting and localizing specification violations.

All test samples and result entries are consistently named and aligned with the corresponding source files in the `TBFV4J-CSC-Dataset` directory. This facilitates easy mapping between program logic, test outcomes, and counterexamples, and supports result reproducibility and further analysis.

We hope this dataset can serve as a valuable empirical resource for future research on testing-based formal verification methods and contribute to the reproducibility and transparency of our study.
